{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import scipy.stats as st\n",
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Glass Identification Data Set csv data using the Pandas library\n",
    "filename = 'dataset/glass.data'\n",
    "glass_df = pd.read_csv(filename, names=[\"Id\", \"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\", \"Fe\", \"Type\"])\n",
    "\n",
    "X = glass_df[[\"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ba\", \"Fe\"]].to_numpy(dtype=np.float32)\n",
    "y = glass_df[[\"Ca\"]].to_numpy(dtype=np.float32)\n",
    "\n",
    "N = np.shape(X)[0] # Number of observations\n",
    "M = np.shape(X)[1] # Number of attributes\n",
    "\n",
    "# Since we're training a neural network for regression, we use a \n",
    "# mean square error loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "max_iter = 10000\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_net(net, loss_fn, X, y,\n",
    "                     n_replicates=3, max_iter = 10000, tolerance=1e-6):\n",
    "    # Specify maximum number of iterations for training\n",
    "    logging_frequency = 1000 # display the loss every 1000th iteration\n",
    "    best_final_loss = 1e100\n",
    "    for r in range(n_replicates):\n",
    "        # print('\\n\\t\\tReplicate: {}/{}'.format(r+1, n_replicates))         \n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "        \n",
    "        # Train the network while displaying and storing the loss\n",
    "        # print('\\t\\t{}\\t{}\\t\\t\\t{}'.format('Iter', 'Loss','Rel. loss'))\n",
    "        learning_curve = [] # setup storage for loss at each step\n",
    "        old_loss = 1e6\n",
    "        for i in range(max_iter):\n",
    "            y_est = net(X) # forward pass, predict labels on training set\n",
    "            loss = loss_fn(y_est.flatten(), y.flatten()) # determine loss\n",
    "            loss_value = loss.cpu().data.numpy() #get numpy array instead of tensor\n",
    "            learning_curve.append(loss_value) # record loss for later display\n",
    "            \n",
    "            # Convergence check, see if the percentual loss decrease is within\n",
    "            # tolerance:\n",
    "            p_delta_loss = np.abs(loss_value-old_loss)/old_loss\n",
    "            if p_delta_loss < tolerance: break\n",
    "            old_loss = loss_value\n",
    "            \n",
    "            # display loss with some frequency:\n",
    "            # if (i != 0) & ((i+1) % logging_frequency == 0):\n",
    "                # print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "                # print(print_str)\n",
    "            # do backpropagation of loss and optimize weights \n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            \n",
    "            \n",
    "        # # # display final loss\n",
    "        # print('\\t\\t\\tFinal loss:')\n",
    "        # print_str = '\\t\\t' + str(i+1) + '\\t' + str(loss_value) + '\\t' + str(p_delta_loss)\n",
    "        # print(print_str)\n",
    "        \n",
    "        if loss_value < best_final_loss: \n",
    "            best_net = net\n",
    "            best_final_loss = loss_value\n",
    "            best_learning_curve = learning_curve\n",
    "        \n",
    "    # Return the best curve along with its final loss and learing curve\n",
    "    return best_net, best_final_loss, best_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden_units):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(M, n_hidden_units), #M features to H hiden units\n",
    "                    # 1st transfer function, either Tanh or ReLU:\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(n_hidden_units, 1), # H hidden units to 1 output neuron\n",
    "                    )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_kfold_ANN(X,y,hidden_units,cvf=10):\n",
    "    CV = model_selection.KFold(cvf, shuffle=True, random_state=42)\n",
    "    train_error = np.empty((cvf,len(hidden_units)))\n",
    "    test_error = np.empty((cvf,len(hidden_units)))\n",
    "    f = 0\n",
    "    y = y.squeeze()\n",
    "    k=0\n",
    "    for train_index, test_index in CV.split(X,y):\n",
    "        print(\"\\n\\t Inner fold:\", k)\n",
    "        k+=1\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "    \n",
    "        for h in range(0,len(hidden_units)):\n",
    "            # Construct a network with the current number of hidden units\n",
    "            n_hidden_units = hidden_units[h]\n",
    "            model = Net(n_hidden_units)\n",
    "            model = model.to(device)\n",
    "            net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                       loss_fn,\n",
    "                                                       X=torch.Tensor(X_train).to(device),\n",
    "                                                       y=torch.Tensor(y_train).to(device),\n",
    "                                                       n_replicates=3,\n",
    "                                                       max_iter=max_iter)\n",
    "\n",
    "            # Determine estimated calcium percentage for training and test set\n",
    "            y_train_est = net(torch.Tensor(X_train).to(device)).cpu().data.numpy().flatten()  # prediction of network\n",
    "            y_train = y_train\n",
    "\n",
    "            y_test_est = net(torch.Tensor(X_test).to(device)).cpu().data.numpy().flatten()  # prediction of network\n",
    "            y_test = y_test\n",
    "\n",
    "            # Evaluate training and test performance\n",
    "            train_error[f,h] = np.power(y_train-y_train_est, 2).mean(axis=0)\n",
    "            test_error[f,h] = np.power(y_test-y_test_est, 2).mean(axis=0)\n",
    "    \n",
    "        f=f+1\n",
    "\n",
    "    opt_val_err = np.min(np.mean(test_error,axis=0))\n",
    "    opt_hidden_units = hidden_units[np.argmin(np.mean(test_error,axis=0))]\n",
    "    train_err_vs_hidden_units = np.mean(train_error,axis=0)\n",
    "    test_err_vs_hidden_units = np.mean(test_error,axis=0)\n",
    "    \n",
    "    return opt_val_err, opt_hidden_units, train_err_vs_hidden_units, test_err_vs_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlr_validate(X,y,lambdas,cvf=10):\n",
    "    CV = model_selection.KFold(cvf, shuffle=True)\n",
    "    M = X.shape[1]\n",
    "    w = np.empty((M,cvf,len(lambdas)))\n",
    "    train_error = np.empty((cvf,len(lambdas)))\n",
    "    test_error = np.empty((cvf,len(lambdas)))\n",
    "    f = 0\n",
    "    y = y.squeeze()\n",
    "    for train_index, test_index in CV.split(X,y):\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        \n",
    "        # precompute terms\n",
    "        Xty = X_train.T @ y_train\n",
    "        XtX = X_train.T @ X_train\n",
    "        for l in range(0,len(lambdas)):\n",
    "            # Compute parameters for current value of lambda and current CV fold\n",
    "            # note: \"linalg.lstsq(a,b)\" is substitue for Matlab's left division operator \"\\\"\n",
    "            lambdaI = lambdas[l] * np.eye(M)\n",
    "            lambdaI[0,0] = 0 # remove bias regularization\n",
    "            w[:,f,l] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "            # Evaluate training and test performance\n",
    "            train_error[f,l] = np.power(y_train-X_train @ w[:,f,l].T,2).mean(axis=0)\n",
    "            test_error[f,l] = np.power(y_test-X_test @ w[:,f,l].T,2).mean(axis=0)\n",
    "    \n",
    "        f=f+1\n",
    "\n",
    "    opt_val_err = np.min(np.mean(test_error,axis=0))\n",
    "    opt_lambda = lambdas[np.argmin(np.mean(test_error,axis=0))]\n",
    "    train_err_vs_lambda = np.mean(train_error,axis=0)\n",
    "    test_err_vs_lambda = np.mean(test_error,axis=0)\n",
    "    mean_w_vs_lambda = np.squeeze(np.mean(w,axis=1))\n",
    "    \n",
    "    return opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Outter fold: 0\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 1\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 2\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 3\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 4\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 5\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 6\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 7\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 8\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n",
      "\n",
      " Outter fold: 9\n",
      "\n",
      "\t Inner fold: 0\n",
      "\n",
      "\t Inner fold: 1\n",
      "\n",
      "\t Inner fold: 2\n",
      "\n",
      "\t Inner fold: 3\n",
      "\n",
      "\t Inner fold: 4\n",
      "\n",
      "\t Inner fold: 5\n",
      "\n",
      "\t Inner fold: 6\n",
      "\n",
      "\t Inner fold: 7\n",
      "\n",
      "\t Inner fold: 8\n",
      "\n",
      "\t Inner fold: 9\n"
     ]
    }
   ],
   "source": [
    "K_outter = 10\n",
    "K_inner = 10\n",
    "CV = model_selection.KFold(K_outter, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables\n",
    "Error_train = np.empty((K_outter,1))\n",
    "Error_test = np.empty((K_outter,1))\n",
    "\n",
    "Error_train_ANN = np.empty((K_outter,1))\n",
    "Error_test_ANN = np.empty((K_outter,1))\n",
    "\n",
    "Error_train_rlr = np.empty((K_outter,1))\n",
    "Error_test_rlr = np.empty((K_outter,1))\n",
    "\n",
    "Error_train_baseline = np.empty((K_outter,1))\n",
    "Error_test_baseline = np.empty((K_outter,1))\n",
    "\n",
    "Optimal_h_history = np.empty((K_outter,1))\n",
    "Optimal_lambda_history = np.empty((K_outter,1))\n",
    "\n",
    "w_rlr = np.empty((M,K_outter))\n",
    "\n",
    "yhat_ANN = []\n",
    "yhat_lr = []\n",
    "yhat_baseline = []\n",
    "y_true = []\n",
    "\n",
    "# Numbers of hidden units\n",
    "hidden_units = [1, 16, 64, 256]#, 128, 256]\n",
    "# Values of lambda\n",
    "lambdas = np.power(10.,range(-5,9))\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    print(\"\\n Outter fold:\", k)\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]   \n",
    "    \n",
    "    opt_val_err, opt_h, train_err_vs_lambda, test_err_vs_lambda = inner_kfold_ANN(X_train, y_train, hidden_units, K_inner)\n",
    "    # # Build the optimal ANN, on the entire training set\n",
    "    n_hidden_units = opt_h\n",
    "    model = Net(n_hidden_units)\n",
    "    model = model.to(device)\n",
    "\n",
    "    net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                       loss_fn,\n",
    "                                                       X=torch.Tensor(X_train).to(device),\n",
    "                                                       y=torch.Tensor(y_train).to(device),\n",
    "                                                       n_replicates=3,\n",
    "                                                       max_iter=max_iter)\n",
    "    \n",
    "    # Determine estimated calcium percentage for training and test set\n",
    "    y_train_est_ANN = net(torch.Tensor(X_train).to(device)).cpu().data.numpy()  # prediction of network\n",
    "    y_test_est_ANN = net(torch.Tensor(X_test).to(device)).cpu().data.numpy()  # prediction of network\n",
    "\n",
    "    # Build the linear model regression \n",
    "    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train, y_train, lambdas, K_inner)\n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = opt_lambda * np.eye(M)\n",
    "    lambdaI[0,0] = 0 # Do no regularize the bias term\n",
    "    w_rlr[:,k] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n",
    "\n",
    "    y_train_est_lr = X_train @ w_rlr[:,k]\n",
    "    y_train_est_lr = y_train_est_lr.reshape(y_train_est_lr.shape[0],1)\n",
    "\n",
    "    y_test_est_lr = X_test @ w_rlr[:,k]\n",
    "    y_test_est_lr = y_test_est_lr.reshape(y_test_est_lr.shape[0],1)\n",
    "\n",
    "    # Compute mean squared error for ANN\n",
    "    Error_train_ANN[k] = np.square(y_train-y_train_est_ANN).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test_ANN[k] = np.square(y_test-y_test_est_ANN).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    Error_train_rlr[k] = np.square(y_train-y_train_est_lr).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test_rlr[k] = np.square(y_test-y_test_est_lr).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    # Compute mean squared error without using the input data at all\n",
    "    Error_train_baseline[k] = np.square(y_train-y_train.mean()).sum(axis=0)/y_train.shape[0]\n",
    "    Error_test_baseline[k] = np.square(y_test-y_test.mean()).sum(axis=0)/y_test.shape[0]\n",
    "\n",
    "    yhat_ANN.append(y_test_est_ANN)\n",
    "    yhat_lr.append(y_test_est_lr)\n",
    "    yhat_baseline.append(np.ones(y_test.shape)*y_test.mean())\n",
    "    y_true.append(y_test)\n",
    "\n",
    "    Optimal_h_history[k] = opt_h\n",
    "    Optimal_lambda_history[k] = opt_lambda\n",
    "    k += 1\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "yhat_ANN = np.concatenate(yhat_ANN)\n",
    "yhat_baseline = np.concatenate(yhat_baseline)\n",
    "yhat_lr = np.concatenate(yhat_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN                                      LR                        Baseline                 \n",
      "h_i                           lambda_i                 \n",
      "256.0    0.8497361540794373   1e-05    0.008888862143995705 3.050967216491699\n",
      "16.0     0.7610147595405579   0.1      0.013513611690812393 0.9661083221435547\n",
      "64.0     0.5633013248443604   0.1      0.011262327097683562 1.721995234489441\n",
      "64.0     0.26031240820884705  0.1      0.007214668300234467 2.149632215499878\n",
      "256.0    0.31309789419174194  1e-05    0.007014191001798834 0.17918002605438232\n",
      "256.0    0.32943740487098694  0.01     0.004887338394325425 0.6483568549156189\n",
      "256.0    0.24517247080802917  1e-05    0.011010279995245767 2.146726608276367\n",
      "64.0     0.6505647301673889   0.01     0.010358637367401783 3.70025634765625\n",
      "256.0    1.2991091012954712   1e-05    0.01502062411502698 2.8127002716064453\n",
      "64.0     2.453014850616455    1e-05    0.024593018270753227 1.8010098934173584\n"
     ]
    }
   ],
   "source": [
    "print(\"{:<40} {:<25} {:<25}\".format(\"ANN\", \"LR\", \"Baseline\"))\n",
    "print(\"{:<28}  {:<25}\".format(\"h_i\", \"lambda_i\"))\n",
    "for i in range(K_outter):\n",
    "    print(\"{:<8} {:<20} {:<8} {:<15} {:<15}\".format(Optimal_h_history[i][0], Error_test_ANN[i][0], Optimal_lambda_history[i][0], Error_test_rlr[i][0], Error_test_baseline[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.]\n",
      " [16.]]\n"
     ]
    }
   ],
   "source": [
    "print(Optimal_h_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confidence interval and p-value for ANN vs baseline in setup I are:\n",
      "Confidence interval: (array([-1.96732499]), array([-0.06081783]))\n",
      "p-value: [0.00619506]\n",
      "\n",
      "The confidence interval and p-value for lr vs baseline in setup I are:\n",
      "Confidence interval: (array([0.95585195]), array([3.03441415]))\n",
      "p-value: [1.25657417e-06]\n",
      "\n",
      "The confidence interval and p-value for ANN vs lr in setup I are:\n",
      "Confidence interval: (array([0.3431698]), array([1.61895348]))\n",
      "p-value: [8.83442062e-05]\n"
     ]
    }
   ],
   "source": [
    "# SETUP I: ANN vs baseline\n",
    "alpha = 0.01\n",
    "\n",
    "z_ANN = np.abs(y_true - yhat_ANN ) ** 2\n",
    "z_baseline = np.abs(y_true - yhat_baseline ) ** 2\n",
    "z_lr = np.abs(y_true - yhat_lr) ** 2\n",
    "\n",
    "\n",
    "z = z_ANN - z_baseline\n",
    "CI_setupI_ANN_baseline = st.t.interval(1-alpha, len(z), loc=np.mean(z), scale=st.sem(z))  # Confidence interval\n",
    "p_setupI_ANN_baseline = 2*st.t.cdf( -np.abs( np.mean(z) )/st.sem(z), df=len(z)-1)  # p-value\n",
    "print(\"The confidence interval and p-value for ANN vs baseline in setup I are:\")\n",
    "print(\"Confidence interval:\", CI_setupI_ANN_baseline)\n",
    "print(\"p-value:\", p_setupI_ANN_baseline)\n",
    "\n",
    "z = z_baseline - z_lr\n",
    "CI_setupI_lr_baseline = st.t.interval(1-alpha, len(z), loc=np.mean(z), scale=st.sem(z))  # Confidence interval\n",
    "p_setupI_lr_baseline = 2*st.t.cdf( -np.abs( np.mean(z) )/ st.sem(z), df=len(z)-1)  # p-value\n",
    "print(\"\\nThe confidence interval and p-value for lr vs baseline in setup I are:\")\n",
    "print(\"Confidence interval:\", CI_setupI_lr_baseline)\n",
    "print(\"p-value:\", p_setupI_lr_baseline)\n",
    "\n",
    "z = z_ANN - z_lr\n",
    "CI_setupI_ANN_lr = st.t.interval(1-alpha, len(z), loc=np.mean(z), scale=st.sem(z))  # Confidence interval\n",
    "p_setupI_ANN_lr = 2*st.t.cdf( -np.abs( np.mean(z) )/st.sem(z), df=len(z)-1)  # p-value\n",
    "print(\"\\nThe confidence interval and p-value for ANN vs lr in setup I are:\")\n",
    "print(\"Confidence interval:\", CI_setupI_ANN_lr)\n",
    "print(\"p-value:\", p_setupI_ANN_lr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-06])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale=st.sem(z_lr - z_baseline)\n",
    "st.sem(z_lr - z_baseline) + (np.ones(scale.shape) * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lazar\\AppData\\Local\\Temp/ipykernel_19044/2712021192.py:13: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  most_common_hidden_units = int(st.mode(Optimal_h_history).mode[0][0])\n",
      "C:\\Users\\lazar\\AppData\\Local\\Temp/ipykernel_19044/2712021192.py:14: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  most_common_lambda = st.mode(Optimal_lambda_history).mode[0][0]\n"
     ]
    }
   ],
   "source": [
    "def correlated_ttest(r, rho, alpha=0.05):\n",
    "    rhat = np.mean(r)\n",
    "    shat = np.std(r)\n",
    "    J = len(r)\n",
    "    sigmatilde = shat * np.sqrt(1 / J + rho / (1 - rho))\n",
    "\n",
    "    CI = st.t.interval(1 - alpha, df=J - 1, loc=rhat, scale=sigmatilde)  # Confidence interval\n",
    "    p = 2*st.t.cdf(-np.abs(rhat) / sigmatilde, df=J - 1)  # p-value\n",
    "    return p, CI\n",
    "\n",
    "loss = 2\n",
    "\n",
    "most_common_hidden_units = int(st.mode(Optimal_h_history).mode[0][0])\n",
    "most_common_lambda = st.mode(Optimal_lambda_history).mode[0][0]\n",
    "\n",
    "K = 5\n",
    "m = 1\n",
    "J = 0\n",
    "\n",
    "r_ANN_baseline = []\n",
    "r_lr_baseline = []\n",
    "r_ANN_lr = []\n",
    "\n",
    "CV = model_selection.KFold(n_splits=K,shuffle=True, random_state = 43)\n",
    "\n",
    "for dm in range(m):\n",
    "    y_true = []\n",
    "    yhat = []\n",
    "\n",
    "    for train_index, test_index in CV.split(X):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train)\n",
    "        y_train_tensor = torch.tensor(y_train)\n",
    "        X_test_tensor = torch.tensor(X_test)\n",
    "        y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "        model_baseline = np.mean(y_test)\n",
    "        \n",
    "        model_lr = lm.Ridge(alpha=most_common_lambda).fit(X_train,y_train.squeeze()) # Linear least squares with l2 regularization.\n",
    "\n",
    "        model_ANN = Net(most_common_hidden_units)\n",
    "        model_ANN = model_ANN.to(device)\n",
    "\n",
    "        net, final_loss, learning_curve = train_neural_net(model_ANN,\n",
    "                                                       loss_fn,\n",
    "                                                       X=X_train_tensor.to(device),\n",
    "                                                       y=y_train_tensor.to(device),\n",
    "                                                       n_replicates=3,\n",
    "                                                       max_iter=max_iter)\n",
    "\n",
    "        \n",
    "        # Determine estimated regression value for test set\n",
    "        yhat_baseline  = np.ones((y_test.shape[0],1))*model_baseline.squeeze()  \n",
    "        yhat_lr =  model_lr.predict(X_test).reshape(-1,1)\n",
    "        yhat_ANN = net(X_test_tensor.to(device)).cpu().detach().numpy()\n",
    "\n",
    "        yhat.append( np.concatenate([yhat_baseline, yhat_ANN], axis=1) )\n",
    "        y_true.append(y_test)        \n",
    "\n",
    "        r_ANN_baseline.append(np.mean( np.abs( yhat_baseline-y_test ) ** loss - np.abs( yhat_ANN-y_test) ** loss ))\n",
    "        r_lr_baseline.append(np.mean( np.abs( yhat_baseline-y_test ) ** loss - np.abs( yhat_lr-y_test) ** loss ))\n",
    "        r_ANN_lr.append(np.mean( np.abs( yhat_ANN-y_test ) ** loss - np.abs( yhat_lr-y_test) ** loss ))\n",
    "\n",
    "# Initialize parameters and run test appropriate for setup II\n",
    "alpha = 0.05\n",
    "rho = 1/K\n",
    "p_setupII_ANN_baseline, CI_setupII_ANN_baseline = correlated_ttest(r_ANN_baseline, rho, alpha=alpha)\n",
    "p_setupII_lr_baseline, CI_setupII_lr_baseline = correlated_ttest(r_lr_baseline, rho, alpha=alpha)\n",
    "p_setupII_ANN_lr, CI_setupII_ANN_lr = correlated_ttest(r_ANN_lr, rho, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6260747689719481 (-2.43952840008522, 3.582562908039198)\n",
      "0.01844106166744855 (0.5480507093570213, 3.4063094441014785)\n",
      "0.11147825029970564 (-0.5114210622138637, 3.3227468507117885)\n"
     ]
    }
   ],
   "source": [
    "print(p_setupII_ANN_baseline, CI_setupII_ANN_baseline)\n",
    "print(p_setupII_lr_baseline, CI_setupII_lr_baseline)\n",
    "print(p_setupII_ANN_lr, CI_setupII_ANN_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e12b95feb72a2bcb1940fe7fb8289dcc3975b1b83ca4e8ecd400d3b43db31fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
